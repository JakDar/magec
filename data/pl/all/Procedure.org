#+TITLE: Preprocessing
* Outline
1. Remove markup
2. Split into small documents (if needed)
3. Group into sizable documents
4. [[Preprocess text]]
  
* Preprocess text
** Per Dataset
1. Remove inside of parenthesis (ppc uses this a lot)
2. Checkout PPC popular words
3. Sentence tokenize (remember about hard cases!)
** Common
*** Stage 1
[[file:stage1.sh][stage1.sh]]
1. Moses punctuation normalization
2. Normalize numbers (make them ones ;)
*** Stage 2
[[file:stage2.sh][stage2.sh]]
1. Trim too long  <= 10 & < 150
2. Take with most words in dict
   - enchant filter
**** TODO /Took probably a little too much - this should be after tokenization/, to review
*** Stage 3
[[file:stage3.sh][stage3.sh]]
3. Trim too short
4. Word tokenize
